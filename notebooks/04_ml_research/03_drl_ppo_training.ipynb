{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf41e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data handling, the DRL agent,\n",
    "# our custom environment, and path management.\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "from qmind_quant.ml_models.environments.trading_env import TradingEnv\n",
    "from qmind_quant.config.paths import FEATURES_DATA_DIR, MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fabe19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating the environment...\n",
      "Environment created successfully.\n"
     ]
    }
   ],
   "source": [
    "# We'll train the agent on a single stock (AAPL) to keep the process fast and simple.\n",
    "print(\"Loading data and creating the environment...\")\n",
    "feature_file = FEATURES_DATA_DIR / \"ml_feature_data.parquet\"\n",
    "df = pd.read_parquet(feature_file)\n",
    "\n",
    "# Filter for a single ticker for training\n",
    "aapl_df = df[df['ticker'] == 'AAPL'].reset_index(drop=True)\n",
    "\n",
    "# Create an instance of our custom trading environment\n",
    "env = TradingEnv(df=aapl_df)\n",
    "print(\"Environment created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecebd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "--- Starting DRL Agent Training ---\n",
      "Logging to ./drl_tensorboard_logs/ppo/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81       |\n",
      "|    ep_rew_mean     | 2e+04    |\n",
      "| time/              |          |\n",
      "|    fps             | 2688     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 81         |\n",
      "|    ep_rew_mean          | 1.88e+04   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1908       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00530415 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 2.26e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.73e+06   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00532   |\n",
      "|    value_loss           | 2.38e+07   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 81         |\n",
      "|    ep_rew_mean          | 1.93e+04   |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1805       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00574297 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 8.59e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.39e+07   |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0055    |\n",
      "|    value_loss           | 2.3e+07    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 81           |\n",
      "|    ep_rew_mean          | 1.97e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1792         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012467167 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.000178     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+07     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 2.55e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 81           |\n",
      "|    ep_rew_mean          | 2.07e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1797         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019514984 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.000334     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+07     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00201     |\n",
      "|    value_loss           | 2.54e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 81           |\n",
      "|    ep_rew_mean          | 2.19e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1791         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025260272 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.000754     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.56e+07     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 2.94e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 81            |\n",
      "|    ep_rew_mean          | 2.27e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1734          |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00076121406 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.01         |\n",
      "|    explained_variance   | 0.000877      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+07       |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00136      |\n",
      "|    value_loss           | 2.89e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 81          |\n",
      "|    ep_rew_mean          | 2.47e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1697        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001162709 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.997      |\n",
      "|    explained_variance   | 0.00172     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.41e+07    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 3.12e+07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 81          |\n",
      "|    ep_rew_mean          | 2.53e+04    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1681        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001190972 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | 0.00222     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.06e+07    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    value_loss           | 3.68e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 81           |\n",
      "|    ep_rew_mean          | 2.66e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1676         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003011763 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.961       |\n",
      "|    explained_variance   | 0.00242      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.26e+07     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000925    |\n",
      "|    value_loss           | 3.7e+07      |\n",
      "------------------------------------------\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Initialize the PPO model. 'MlpPolicy' uses a standard neural network.\n",
    "# We also set up TensorBoard logging to visualize the training process later.\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./drl_tensorboard_logs/ppo/\"\n",
    ")\n",
    "\n",
    "# Train the agent. 20,000 timesteps is a small number for a quick initial run.\n",
    "# A production model would train for millions of timesteps.\n",
    "print(\"\\n--- Starting DRL Agent Training ---\")\n",
    "model.learn(total_timesteps=20000)\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e093e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DRL agent saved to: /Users/enisyasaroglu/qmind_quant_platform/qmind_quant/ml_models/models/drl_ppo_v1.zip\n"
     ]
    }
   ],
   "source": [
    "# Save the learned policy and model weights to a file.\n",
    "model_path = MODELS_DIR / \"drl_ppo_v1.zip\"\n",
    "model.save(model_path)\n",
    "print(f\"\\nDRL agent saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb4abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Trained Agent ---\n",
      "Step: 31, Total Value: 100000.00, Position: 592.84, Cash: 0.00\n",
      "Step: 32, Total Value: 99652.71, Position: 592.84, Cash: 0.00\n",
      "Step: 33, Total Value: 102124.89, Position: 592.84, Cash: 0.00\n",
      "Step: 34, Total Value: 100258.99, Position: 592.84, Cash: 0.00\n",
      "Step: 35, Total Value: 99652.71, Position: 592.84, Cash: 0.00\n",
      "Step: 36, Total Value: 101848.26, Position: 592.84, Cash: 0.00\n",
      "Step: 37, Total Value: 107940.43, Position: 592.84, Cash: 0.00\n",
      "Step: 38, Total Value: 106957.45, Position: 592.84, Cash: 0.00\n",
      "Step: 39, Total Value: 107363.58, Position: 592.84, Cash: 0.00\n",
      "Step: 40, Total Value: 107563.72, Position: 592.84, Cash: 0.00\n",
      "Step: 41, Total Value: 108640.89, Position: 592.84, Cash: 0.00\n",
      "Step: 42, Total Value: 107892.33, Position: 592.84, Cash: 0.00\n",
      "Step: 43, Total Value: 109796.13, Position: 592.84, Cash: 0.00\n",
      "Step: 44, Total Value: 110473.96, Position: 592.84, Cash: 0.00\n",
      "Step: 45, Total Value: 111823.73, Position: 592.84, Cash: 0.00\n",
      "Step: 46, Total Value: 111894.45, Position: 592.84, Cash: 0.00\n",
      "Step: 47, Total Value: 111912.13, Position: 592.84, Cash: 0.00\n",
      "Step: 48, Total Value: 112601.74, Position: 592.84, Cash: 0.00\n",
      "Step: 49, Total Value: 113373.89, Position: 592.84, Cash: 0.00\n",
      "Step: 50, Total Value: 112519.22, Position: 592.84, Cash: 0.00\n",
      "Step: 51, Total Value: 110149.79, Position: 592.84, Cash: 0.00\n",
      "Step: 52, Total Value: 111976.97, Position: 592.84, Cash: 0.00\n",
      "Step: 53, Total Value: 111982.87, Position: 592.84, Cash: 0.00\n",
      "Step: 54, Total Value: 112159.69, Position: 592.84, Cash: 0.00\n",
      "Step: 55, Total Value: 112749.09, Position: 592.84, Cash: 0.00\n",
      "Step: 56, Total Value: 113314.93, Position: 592.84, Cash: 0.00\n",
      "Step: 57, Total Value: 114364.10, Position: 592.84, Cash: 0.00\n",
      "Step: 58, Total Value: 114552.72, Position: 592.84, Cash: 0.00\n",
      "Step: 59, Total Value: 115448.62, Position: 592.84, Cash: 0.00\n",
      "Step: 60, Total Value: 114629.35, Position: 592.84, Cash: 0.00\n",
      "Step: 61, Total Value: 116049.82, Position: 592.84, Cash: 0.00\n",
      "Step: 62, Total Value: 113827.74, Position: 592.84, Cash: 0.00\n",
      "Step: 63, Total Value: 122097.21, Position: 592.84, Cash: 0.00\n",
      "Step: 64, Total Value: 125586.55, Position: 592.84, Cash: 0.00\n",
      "Step: 65, Total Value: 126276.16, Position: 592.84, Cash: 0.00\n",
      "Step: 66, Total Value: 125244.70, Position: 592.84, Cash: 0.00\n",
      "Step: 67, Total Value: 127708.46, Position: 592.84, Cash: 0.00\n",
      "Step: 68, Total Value: 126305.64, Position: 592.84, Cash: 0.00\n",
      "Step: 69, Total Value: 123588.43, Position: 592.84, Cash: 0.00\n",
      "Step: 70, Total Value: 122297.63, Position: 592.84, Cash: 0.00\n",
      "Step: 71, Total Value: 122680.74, Position: 592.84, Cash: 0.00\n",
      "Step: 72, Total Value: 123228.91, Position: 592.84, Cash: 0.00\n",
      "Step: 73, Total Value: 125692.64, Position: 592.84, Cash: 0.00\n",
      "Step: 74, Total Value: 126193.65, Position: 592.84, Cash: 0.00\n",
      "Step: 75, Total Value: 124142.48, Position: 592.84, Cash: 0.00\n",
      "Step: 76, Total Value: 127755.61, Position: 592.84, Cash: 0.00\n",
      "Step: 77, Total Value: 129830.34, Position: 592.84, Cash: 0.00\n",
      "Step: 78, Total Value: 130584.79, Position: 592.84, Cash: 0.00\n",
      "Step: 79, Total Value: 133408.09, Position: 592.84, Cash: 0.00\n",
      "Step: 80, Total Value: 134280.44, Position: 592.84, Cash: 0.00\n",
      "Step: 81, Total Value: 134787.31, Position: 592.84, Cash: 0.00\n",
      "Step: 82, Total Value: 137321.80, Position: 592.84, Cash: 0.00\n",
      "Step: 83, Total Value: 134133.07, Position: 592.84, Cash: 0.00\n",
      "Step: 84, Total Value: 135883.62, Position: 592.84, Cash: 0.00\n",
      "Step: 85, Total Value: 138158.77, Position: 592.84, Cash: 0.00\n",
      "Step: 86, Total Value: 138406.33, Position: 592.84, Cash: 0.00\n",
      "Step: 87, Total Value: 134905.21, Position: 592.84, Cash: 0.00\n",
      "Step: 88, Total Value: 132134.95, Position: 592.84, Cash: 0.00\n",
      "Step: 89, Total Value: 132211.56, Position: 592.84, Cash: 0.00\n",
      "Step: 90, Total Value: 132005.28, Position: 592.84, Cash: 0.00\n",
      "Step: 91, Total Value: 132624.15, Position: 592.84, Cash: 0.00\n",
      "Step: 92, Total Value: 128810.65, Position: 592.84, Cash: 0.00\n",
      "Step: 93, Total Value: 128191.77, Position: 592.84, Cash: 0.00\n",
      "Step: 94, Total Value: 128468.79, Position: 592.84, Cash: 0.00\n",
      "Step: 95, Total Value: 128633.83, Position: 592.84, Cash: 0.00\n",
      "Step: 96, Total Value: 128963.90, Position: 592.84, Cash: 0.00\n",
      "Step: 97, Total Value: 130897.19, Position: 592.84, Cash: 0.00\n",
      "Step: 98, Total Value: 128704.56, Position: 592.84, Cash: 0.00\n",
      "Step: 99, Total Value: 129588.67, Position: 592.84, Cash: 0.00\n",
      "Step: 100, Total Value: 123346.79, Position: 592.84, Cash: 0.00\n",
      "Step: 101, Total Value: 122144.37, Position: 592.84, Cash: 0.00\n",
      "Step: 102, Total Value: 123670.95, Position: 592.84, Cash: 0.00\n",
      "Step: 103, Total Value: 125728.02, Position: 592.84, Cash: 0.00\n",
      "Step: 104, Total Value: 127455.00, Position: 592.84, Cash: 0.00\n",
      "Step: 105, Total Value: 128363.75, Position: 592.84, Cash: 0.00\n",
      "Step: 106, Total Value: 130570.72, Position: 592.84, Cash: 0.00\n",
      "Step: 107, Total Value: 130836.25, Position: 592.84, Cash: 0.00\n",
      "Step: 108, Total Value: 132606.55, Position: 592.84, Cash: 0.00\n",
      "Step: 109, Total Value: 133391.38, Position: 592.84, Cash: 0.00\n",
      "Step: 110, Total Value: 133296.97, Position: 592.84, Cash: 0.00\n",
      "Step: 111, Total Value: 133662.82, Position: 592.84, Cash: 0.00\n",
      "--- Evaluation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Run a full backtest episode using the trained agent's policy to see\n",
    "# how it performs on the training data.\n",
    "print(\"\\n--- Evaluating Trained Agent ---\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Use the model to predict the best action in the current state\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Take the action in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Print the state at each step\n",
    "    env.render()\n",
    "\n",
    "print(\"--- Evaluation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qmind_quant_platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
